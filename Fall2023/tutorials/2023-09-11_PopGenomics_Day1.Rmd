---
title: "Ecological Genomics Tutorials: Population & Landscape Genomics 1"
date: 'September 11, 2023'
output:
  prettydoc::html_pretty:
    theme: cayman
fontsize: 18pt
---

# Learning Objectives for 09/11/23

1.  To get background on the ecology of Red spruce (*Picea rubens*), and the experimental design of the exome capture data
2.  To understand the general work flow or "pipeline" for processing and analyzing the exome capture sequence data
3.  To visualize and interpret Illumina data quality (what is a fastq file; what are Phred scores?).
4.  To learn how to make/write a bash script, and how to use bash commands to process files in batches
5.  To trim the reads based on base quality scores in preparation for mapping to the reference genome

## 1. Red spruce, *Picea rubens*

<img src="https://static.inaturalist.org/photos/5362103/large.jpg?1544110582" width="550" height="350"/> <img src="https://www.researchgate.net/profile/Adam_Rollins/publication/33756615/figure/fig1/AS:309876237586432@1450891660815/The-range-of-red-spruce-USGS-1999_W640.jpg" width="270" height="350"/>

Red spruce is a coniferous tree that plays a prominent role in montane communities throughout the Appalachians. It thrives in the cool, moist climates of the high elevation mountains of the Appalachians and northward along the coastal areas of Atlantic Canada. In the low-latitude trailing edge of the range, populations are highly fragmented and isolated on mountaintops. These "island" populations are remnants of spruce forests that covered the southern U.S. glaciers extended as far south as Long Island, NY. As the climate warmed at the end of the Pleistocene (\~20K years ago), red spruce retreated upward in elevation to these mountaintop refugia, where they are now highly isolated from other such stands and from the core of the range further north.

Because of its preference for cool, moist climates, red spruce shows climate sensitivities that may make it especially vulnerable to climate change. This makes assessing the amount and distribution of genetic diversity across the landscape of red spruce an important conservation issue! Ultimately, we want to use genomic insights to help inform conservation biologists working to restore red spruce and evaluate the potential for assisted migration (a form of human-mediated dispersal) to offset the loss of adaptation under climate change. A close partner in this effort is the [Nature Conservancy](https://www.nature.org/en-us/about-us/where-we-work/united-states/west-virginia/stories-in-west-virginia/sprucing-things-up-a-bit/) and the Central Appalachian Spruce Restoration Initiative ([CASRI](http://restoreredspruce.org)) -- a multi-partner group dedicated to restoring and enhancing red spruce populations to promote their resilience under climate change.

![](http://restoreredspruce.org/wp/wp-content/uploads/2019/09/cropped-New-logo-CASRI-header-1.jpg)

Some videos of the collaboration between UVM and CASRI on red spruce restoration:

-   [Building Resilience](https://youtu.be/Ld1EvG9cZN8?si=f7kRufegkeJNWsLJ)

-   [Seeds of Hope](https://youtu.be/FYKbjXB4cHs)

Since 2015, the Keller Lab has been studying the genetic basis of climate adaptation across the entire distribution of *P. rubens*. Our main goal is to use genomics to aid conservation of red spruce under climate change through a better understanding of **(1) how genetic diversity diversity is distributed across the range and how that reflects the demographic history of population expansions, bottlenecks, gene flow, and divergence** and **(2) identify regions of the genome that show evidence of adaptation in response to abiotic climate gradients.** We hope to use this information to inform areas of the range most likely to experience climate mal-adaptation, and to help guide mitigation strategies such as sourcing seed for restoration and assisted migration.

## Summary of prior population genomics research on red spruce

Our recent prior work funded by NSF (2017-2022) focused on early-life fitness of seedlings in response to genomic variation and climate adaptation. That work sampled seeds and needle tissue from 340 mother trees at 65 populations spread throughout the range, and generated population genomic data through exome capture sequencing. Based on these data, some of the insights we gleaned were:

-   Red spruce has very low genetic diversity and has an Ne that has been declining for thousands of years [Capblancq et al. 2020](https://onlinelibrary.wiley.com/doi/full/10.1111/eva.12985)
-   Populations are genetically differentiated into 3 geographically separated ancestry groups in the north (core) mid-latitude (margin) and southern (edge) regions of its range [Capblancq et al. 2020](https://onlinelibrary.wiley.com/doi/full/10.1111/eva.12985))
-   A local population's level of genetic diversity and its frequency of deleterious mutations (aka, "genetic load") were related to how well seedlings survived and grew under greenhouse conditions [Capblancq et al. 2021](https://link.springer.com/article/10.1007/s10592-021-01378-7)
-   Seedling growth traits in common garden experiments showed heritable genetic variation and trait divergence among the 3 ancestry groups [Prakash et al. 2022](https://royalsocietypublishing.org/doi/full/10.1098/rstb.2021.0008)
-   Certain genomic regions showed strong allele frequency clines along climatic gradients indicative of selection, often related to abiotic stress (heat and drought) [Capblancq et al. 2023](https://nph.onlinelibrary.wiley.com/doi/full/10.1111/nph.18465)
-   There was a hint in some of the results that hybridization with black spruce might be playing a role in some of the above results, but we lacked genomic data from black spruce to nail that down.

*Let's brainstorm about some issues these data couldn't address, or where the inference of population history or climate adaptation were constrained by aspects of the experimental design?*. **What questions would you want to ask next?**

## A new dataset for analysis

The data we'll be analyzing here consist of exome-capture data for adult red spruce growing in a [provenance trial in northern New Hampshire near the town of Colebrook](https://goo.gl/maps/4LSPvpN2RdVbUuH78). Here are the details:

-   95 individuals sampled from 12 populations across the range (N=190 red spruce)
-   Individuals were grown from seed and planted out into the provenance trial as 2 year old seedlings in 1960
-   Multiple studies have assessed survival, growth (height DBH), and cold tolerance of these individuals at multiple time points over the last 60 years
-   Needle tissue was sampled from surviving red spruce in the trial in May 2020 for genomic DNA
-   We also sampled 18 black spruce individuals from natural stands in 2 locations distant from red spruce's range (MN and MI). These will be useful for detecting black spruce ancestry in the red spruce populations, if it exists
-   We used the same exome-capture probe set as Capblancq et al. (2020)
-   Exome-capture was designed based on transcriptomes from multiple tissues and developmental stages in the related species, white spruce (*P. glauca*).\
-   Bait design used 2 transcriptomes previously assembled by [Rigault et al. (2011)](http://www.plantphysiol.org/content/157/1/14.full) and [Yeaman et al. (2014)](https://nph.onlinelibrary.wiley.com/doi/full/10.1111/nph.12819).
-   A total of 80,000 120 bp probes were designed, including 75,732 probes within or overlapping exomic regions, and an additional 4,268 probes in intergenic regions.
-   Each probe was required to represent a single blast hit to the *P. glauca* reference genome of at least 90bp long and 85% identity, covering **38,570 unigenes**.\
-   Libraries were made by random mechanical shearing of DNA (250 ng -1ug) to an average size of 400 bp followed by end-repair reaction, ligation of an adenine residue to the 3'-end of the blunt-end fragments to allow the ligation of barcoded adapters, and PCR-amplification of the library. SureSelect probes (Agilent Technologies: Santa Clara, CA) were used for solution-based targeted enrichment of pools of 16 libraries, following the SureSelectxt Target Enrichment System for Illumina Paired-End Multiplexed Sequencing Library protocol.\
-   Libraries were sequenced on an Illumina HiSeq X to generate paired-end 150-bp reads.

## 2. The "pipeline"

-   Visualize the quality of raw data (Program: FastQC)

-   Clean raw data (Program: Trimmomatic)

-   Visualize the quality of cleaned data (Program: FastQC)

-   Calculate \#'s of cleaned, high quality reads going into mapping

We'll then use these cleaned reads to align to the reference genome next time so that we can start estimating genomic diversity and population structure.

## 3.-5. Visualize, Clean, and Visualize again

Whenever you get a new batch of NGS data, the first step is to look at the data quality of coming off the sequencer and see if we notice any problems with base quality, sequence length, PCR duplicates, or adapter contamination.

You'll each be in charge of cleaning and visualizing the left (R1) and right (R2) files from a single sample.

We're going to assign each student to analyze all the sequence files from *one population*. (We'll do this in class)

### What is a .fastq file?

[A fastq file is the standard sequence data format for NGS](https://en.wikipedia.org/wiki/FASTQ_format). It contains the sequence of the read itself, the corresponding quality scores for each base, and some meta-data about the read.

The files are big (typically many Gb compressed), so we can't open them completely. Instead, we can peek inside the file using `head`. But size these files are compressed (note the .gz ending in the filenames), and we want them to stay compressed while we peek. Bash has a solution to that called `zcat`. This lets us look at the .gz file without uncompressing it all the way.

The fastq files are in this path: `/netfiles/ecogen/PopulationGenomics/fastq/red_spruce`

```         
cd /netfiles/ecogen/PopulationGenomics/fastq/red_spruce
zcat 2505_9_C_R2.fastq.gz | head -n 4

@A00354:455:HYG3FDSXY:1:1101:3893:1031 2:N:0:CATCAAGT+TACTCCTT
GTGGAAAATCAAAACCCTAATGCTGAAAGGAATCCAAATCAAATAAATATTTTCACCGACCTGTTTCGATGCCAGAATTGTCTGCGCAGAAGACTCGTGAAATTTCGCCAGCAGGTAAAATTAAAAGGCTAGAATTAACCGCTGAAATGGA
+
FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:F:F
```

*Note:* `zcat` lets us open a .gz (gzipped) file; we then "pipe" `|` this output from `zcat` to the `head` command and print just the top 4 lines `-n4`

The fastq file format\*\* has 4 lines for each read:

| Line | Description                                                                                                   |
|-------------|-----------------------------------------------------------|
| 1    | Always begins with '\@' and then information about the read                                                   |
| 2    | The actual DNA sequence                                                                                       |
| 3    | Always begins with a '+' and sometimes the same info in line 1                                                |
| 4    | A string of characters which represent the **quality** scores; always has same number of characters as line 2 |

[Here's a useful reference for understanding Quality (Phred) scores](http://www.drive5.com/usearch/manual/quality_score.html). If P is the probability that a base call is an error, then:

Q = --10 log10(P)

So:

| Phred Quality Score | Probability of incorrect base call | Base call accuracy |
|--------------------|----------------------------------|-------------------|
| 10                  | 1 in 10                            | 90%                |
| 20                  | 1 in 100                           | 99%                |
| 30                  | 1 in 1000                          | 99.9%              |
| 40                  | 1 in 10,000                        | 99.99%             |

*The Phred Q score is translated to ASCII characters so that a two digit number can be represented by a single character.*

```         
 Quality encoding: !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHI
                   |         |         |         |         |
    Quality score: 0........10........20........30........40   
```

*What kind of characters do you want to see in your quality score?*

### Visualize using FastQC

We're going to use [the program FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) (already installed on our server). FastQC looks at the quality collectively across all reads in a sample.

First, let's make a new folder within our home directories that we name `myresults`.

`mkdir ~/myresults/`

Then let's cd into that and make another folder called `fastqc` to hold the outputs from our QC analysis. Do that on your own, just like above. Then cd into the `fastqc` folder and type `pwd` to print the present working directory to screen to prove to yourself you did it right.

The basic FastQC command is like this:

```         
fastqc FILENAME.fastq.gz -o outputdirectory/
```

This will generate an .html output file for each input file you've run.

But, we want to be clever and process multiple files (i.e., ALL files from our population) without having to manually submit each one. We can do this by writing a bash script that contains a loop.

The basic syntax of a bash loops is like this:

```         
cd <location of the input data files>

for name in somefilelist
do
  command1 -options ${name} -moreOptions
  command2 -options ${name} -moreOptions
done
```

Note the use of variable assignment using \${}. We define the word `name` in the "for loop" as the variable of interest, and then call the iterations of it using \${name}. For example, we could use the wildcard character (\*) in a loop to call all files that include the ID code for your population and then pass those filenames in a loop to fastqc. Something like:

```         
cd /netfiles/ecogen/PopulationGenomics/fastq/red_spruce/

for file in <mypop>*fastq.gz

do

 fastqc ${file} -o ~/myresults/fastqc/

done
```

Let's write the above into a script using the Vim text editor at the command line. Type `vim` to get into the editor, then type "i" to enter INSERT mode. You can then type your script (remember to make necessary changes to the "mypop" population code and output path). Lastly, to save the file and quit Vim, hit the ESCAPE key to get out of INSERT mode, followed by `:wq ~/myscripts/fastqc.sh`. This will save the file into your `my scripts/` folder in your home directory.

Back at the command line, you should be able to cd into the `myscripts` folder and `ll` to see your new script!

#### You may find that you need to change the permission on your script to make it executable. Do this using chmod u+x, which changes the permissions to give the user (you) permission to execute (x). Then give it a run!

```         
chmod u+x fastqc.sh    # makes the script "executable" by the "user"
./fastqc.sh             # executes the script
```

It'll take just a couple of minutes per fastq file. Once you've got results, let's look at them by using Filezilla to transfer the folder ~/myresults/fastqc/ over to your laptop. Once complete, you can open the html outputs with a web browser.

*How does the quality look?*

### Clean using Trimmomatic

[We'll use the Trimmomatic program](http://www.usadellab.org/cms/index.php?page=trimmomatic) to clean the reads for each file. The program is already installed on our server.

Since this job is a bit more complicated, we've provided a partially completed example script here: `/netfiles/ecogen/PopulationGenomics/scripts/trim_loop_Fa23.sh` 

1.  `cd` over to the directory where the example script lives
2.  Open and edit the bash script using the program vim.
3.  Edit the file so that you're trimming the fastq files for the population assigned to you; remember to add annotations as notes to yourself!
4.  Save the file in vim to your home directory. To do this, hit <Esc> when you're finished making edits, then `:wq ~/myscripts/trim_loop_Fa23.sh` This will write (w) a new copy of the file over to your ~/myscripts folder and the quit (q) vim.
5.  Change the permissions on your script to make it executable, then run it! (examples below)


This time we use the variable coding to call the name of the R1 read pair, define the name for the second read in the pair (R2), and create a basename that only contains the "pop_ind" part of the name, i.e. AB_05

```         
    R2=${R1/_R1_fastq.gz/_R2_fastq.gz}   # defines the name for the second read in the pair (R2) based on knowing the R1 name (the file names are identifcal except for the R1 vs. R2 designation)
    f=${R1/_R1_fastq.gz/}   # creates a new variable from R1 that has the "_R1_fastq.gz" stripped off
    name=`basename ${f}`   # calls the handy "basename" function to define a new variable containing only the very last part of the filename while stripping off all the path information.  This gets us the "AB_05" bit we want.
```

Here's how it should look (replace AB with your population name):

```         
#!/bin/bash   
 
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq  

for R1 in AB*R1_fastq.gz  

do 
 
    R2=${R1/_R1_fastq.gz/_R2_fastq.gz}
    f=${R1/_R1_fastq.gz/}
    name=`basename ${f}`

    java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticPE \
        -threads 1 \
        -phred33 \
         "$R1" \
         "$R2" \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${name}_R1.cl.pd.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/unpairedcleanreads/${name}_R1.cl.un.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${name}_R2.cl.pd.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/unpairedcleanreads/${name}_R2.cl.un.fq \
        ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:10 \
        LEADING:20 \
        TRAILING:20 \
        SLIDINGWINDOW:6:20 \
        MINLEN:35 
 
done 
```

Trimmomatic performs the cleaning steps in the order they are presented. It's recommended to clip adapter early in the process and clean for length at the end.

The steps and options are [from the Trimmomatic website](http://www.usadellab.org/cms/index.php?page=trimmomatic):

```         
ILLUMINACLIP: Cut adapter and other illumina-specific sequences from the read.
LEADING: Cut bases off the start of a read, if below a threshold quality
TRAILING: Cut bases off the end of a read, if below a threshold quality
SLIDINGWINDOW: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.
MINLEN: Drop the read if it is below a specified length
```

### Visualize again using FastQC

Check the quality of one of your cleaned files using fastqc again.

```         
                # You fill in the blank!  Don't forget to change your input path to the pairedcleaned files!
```

## 6. Mapping cleaned and trimmed reads against the reference genome

Now that we have cleaned and trimmed read pairs, we're ready to map them against the reference genome.

-   We'll be using a reduced reference genome based on selecting only those scaffolds of the full genome reference that contain at least one bait. We've placed it on our server here:

`/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa`

-   The reference genome is based on Norway spruce (*P. abies*) and is available from [congenie.org](http://congenie.org).

-   We'll use the program [bwa](https://github.com/lh3/bwa), which is a very efficient and very well vetted read mapper. Lots of others exist and can be useful to explore for future datasets. We tried several, and for our exome data, bwa seems to be the best

-   We are going to write a bash script together that calls the R1 and R2 reads for each individual in our population, and uses the bwa-mem algorithm to map reads to the reference genome. The resulting output will be a sam file alignment. The basic bwa command we'll use is:

```         
bwa mem -t 1 -M -a ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam
```

where

```         
-t 1 is the number of threads, or computer cpus to use (in this case, just 1)
-M labels a read with a special flag if its mapping is split across >1 contig
-a keeps alignments involving unpaired reads
${ref} specifies the path and filename for the reference genome
${forward} specifies the path and filename for the cleaned and trimmed R1 reads 
${reverse} specifies the path and filename for the cleaned and trimmed R2 reads 
>${output}/BWA/${name}.sam  directs the .sam file to be saved into a directory called BWA
```

Other bwa options detailed here: [bwa manual page](http://bio-bwa.sourceforge.net/bwa.shtml)

Our last steps (with basic syntax) are to: 1. Convert our sam files to the more efficient binary version (bam) and sort them + `sambamba-0.7.1-linux-static view -S --format=bam file.sam -o file.bam`

2.  Get rid of any PCR duplicate sequences (why are these a problem?) and re-sort after removing dups

-   `sambamba-0.7.1-linux-static markdup -r -t 1 file.bam file.rmdup.bam`
-   `samtools sort file.rmdup.bam -o file.sorted.rmdup.bam`

3.  Get some stats on how well the mapping worked

-   `samtools flagstat file.sorted.rmdup.bam | awk 'NR>=5&&NR<=13 {print $1}' | column -x`
-   `samtools depth file.sorted.rmdup.bam | awk '{sum+=$3} END {print sum/NR}`

For this, we'll use a combination of two new programs: [samtools](https://github.com/samtools/samtools) and [sambamba](https://lomereiter.github.io/sambamba/). Samtools was writtend by Heng Li, the same person who wrote bwa, and is a powerful tool for manipulating sam/bam files. Sambamba is derived from samtools, and has been re-coded to increase efficiency (speed). We'll use them both at different steps.

I've put a bash script with the commands to run steps 1-3 above. It's located here:

`/data/scripts/process_bam.sh`

Make a copy of this over to your home directory and use `vim` to edit the paths and population. Then when you're ready to save and quit vim, type `:wq`

Last step, we're going to put these scripts altogether into a "wrapper" that will exectue each of them one after the other, and work for us while we're off getting a coffee or sleeping. :) I'll show you how to code this together in class.

Once your wrapper script is ready, you're going to want to start a `screen`. The `screen` command initiates a new shell window that won't interupt or stop your work if you close your computer, log off the server, or leave the UVM network. Anytime you're running long jobs, you definiteily want to use `screen`.

Using it is easy. Just type `screen` followed by <Enter>. It will take you to a new empyt terminal. You can then start your wrapper bash script and see that it starts running. Once everything looks good, you have to detach from the screen by typing Ctrl-A + Ctrl-D. If you don't do this, you'll lose your work!

When you're ready to check back on the progress of your program, you can recover your screen by typing `screen -r`. That'll re-attach you back to your program!
